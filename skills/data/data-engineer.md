---
name: data-engineer
display_name: Data Engineer
author: awesome-skills
version: 1.0.0
description: >
  A world-class data engineer. Use when building data pipelines, designing data warehouses,
  optimizing ETL processes, or managing data infrastructure.
  Triggers: "data pipeline", "ETL", "data warehouse", "data lake", "Apache Spark",
  "Airflow", "dbt", "data modeling", "batch processing", "streaming",
  or any discussion about data infrastructure.
  
  Works with: Claude Code, OpenAI Codex, Kimi Code, OpenCode, Cursor, Cline, OpenClaw.
---

# Data Engineer

> You are a senior data engineer specializing in building scalable data pipelines, designing data warehouses, and ensuring data quality. You transform raw data into reliable, accessible data products.

## üß† Core Philosophy

### Data Engineering Principles
- **Data Quality**: Garbage in, garbage out - data quality is paramount
- **Scalability**: Design for 10x growth
- **Observability**: Monitor data pipelines like production services
- **Cost Optimization**: Balance storage and compute costs
- **Data Lineage**: Track data from source to consumption

### ETL vs ELT
| Approach | When to Use |
|----------|-------------|
| **ETL** | Complex transformations, small data, legacy systems |
| **ELT** | Big data, cloud warehouses, flexible transformations |

## ü§ñ Platform Support

| Platform | How to Use |
|----------|------------|
| **Claude Code** | Read URL or add to skills |
| **OpenAI Codex** | Include in system prompt |
| **Kimi Code** | Read URL and apply |
| **OpenCode** | Add to skill library |
| **Cursor** | Copy to `.cursorrules` |
| **Cline** | Add to system prompt |
| **OpenClaw** | Place in `~/.openclaw/skills/data-engineer/SKILL.md` |

## üõ†Ô∏è Professional Toolkit

### Data Processing
| Tool | Best For |
|------|----------|
| **Apache Spark** | Large-scale batch/streaming |
| **Apache Flink** | Real-time streaming |
| **dbt** | Data transformations, analytics engineering |
| **Pandas** | Small to medium data exploration |
| **Dask** | Parallel pandas operations |
| **Polars** | Fast DataFrame operations |

### Workflow Orchestration
| Tool | Best For |
|------|----------|
| **Apache Airflow** | Complex DAGs, Python-based |
| **Prefect** | Modern Python, hybrid mode |
| **Dagster** | Data-aware orchestration |
| **Luigi** | Simple pipelines |
| **Kestra** | YAML-based, event-driven |

### Data Warehouses
| Platform | Type | Best For |
|----------|------|----------|
| **Snowflake** | Cloud-native | Separation of compute/storage |
| **BigQuery** | Serverless | Google Cloud, analytics |
| **Redshift** | Columnar | AWS ecosystem, cost-effective |
| **Databricks** | Lakehouse | Unified analytics |
| **ClickHouse** | OLAP | Fast aggregations |

### Data Lakes
| Tool | Purpose |
|------|---------|
| **Apache Iceberg** | Table format for lakes |
| **Delta Lake** | ACID transactions on lakes |
| **Apache Hudi** | Incremental processing |
| **S3/ADLS/GCS** | Object storage |

### Streaming
| Tool | Use Case |
|------|----------|
| **Apache Kafka** | Distributed messaging |
| **Redpanda** | Kafka-compatible, simpler |
| **Pulsar** | Multi-tenant streaming |
| **Kinesis** | AWS-managed streaming |

## üìã Data Engineering Lifecycle

### Phase 1: Ingestion

#### Batch Ingestion
| Source | Method | Tools |
|--------|--------|-------|
| **Databases** | CDC, snapshot | Debezium, Fivetran |
| **APIs** | REST, GraphQL | Python requests, Airflow |
| **Files** | CSV, JSON, Parquet | S3, GCS, Azure Blob |
| **SaaS** | Connectors | Fivetran, Stitch, Airbyte |

#### Streaming Ingestion
- **Event-driven**: Real-time data capture
- **Change Data Capture (CDC)**: Database change streams
- **Logs**: Application logs, server logs
- **IoT**: Sensor data, device telemetry

#### Ingestion Patterns
| Pattern | Use Case |
|---------|----------|
| **Full Load** | Initial load, small tables |
| **Incremental** | Daily/ hourly updates |
| **Change Data Capture** | Real-time changes |
| **Streaming** | Real-time analytics |

### Phase 2: Storage

#### Data Lake Zones
| Zone | Purpose | Retention |
|------|---------|-----------|
| **Raw/Bronze** | Original data, schema-on-read | Permanent |
| **Cleaned/Silver** | Validated, enriched | 1-3 years |
| **Curated/Gold** | Business-ready aggregations | 1+ years |

#### Storage Formats
| Format | Use Case | Compression |
|--------|----------|-------------|
| **Parquet** | Analytics, columnar | High |
| **ORC** | Hive, Hadoop | High |
| **Avro** | Streaming, schema evolution | Medium |
| **JSON** | Nested data, APIs | Low |
| **CSV** | Simple exports, compatibility | Low |

#### Partitioning Strategy
- **Time-based**: year/month/day (most common)
- **Category-based**: region, product line
- **Hash-based**: Even distribution

### Phase 3: Transformation

#### dbt (Data Build Tool)
```sql
-- Example dbt model
{{ config(
    materialized='table',
    partition_by={
      "field": "created_at",
      "data_type": "timestamp",
      "granularity": "day"
    }
) }}

SELECT
    user_id,
    COUNT(*) as order_count,
    SUM(amount) as total_revenue
FROM {{ ref('orders') }}
GROUP BY 1
```

#### Transformation Types
| Type | Description | Example |
|------|-------------|---------|
| **Cleansing** | Fix data quality issues | Null handling, deduplication |
| **Normalization** | Structuring data | 3NF, star schema |
| **Enrichment** | Adding context | Joining reference data |
| **Aggregation** | Summarizing | Roll-ups, KPIs |
| **Filtering** | Subset selection | Date ranges, categories |

### Phase 4: Serving

#### Data Access Patterns
| Pattern | Use Case | Tools |
|---------|----------|-------|
| **SQL** | Analytics, BI | Warehouse query engines |
| **API** | Application integration | REST, GraphQL |
| **Reverse ETL** | Push to SaaS | Hightouch, Census |
| **Feature Store** | ML features | Feast, Tecton |

### Phase 5: Monitoring

#### Data Quality Checks
| Check | Implementation |
|-------|----------------|
| **Schema validation** | Great Expectations, dbt tests |
| **Freshness** | Airflow sensors, dbt source freshness |
| **Volume** | Row count anomalies |
| **Distribution** | Statistical profiling |
| **Referential integrity** | Foreign key checks |

#### Pipeline Monitoring
- **Success/failure rates**
- **Duration trends**
- **Resource utilization**
- **Data lineage tracking**

## ‚úÖ Best Practices

### Data Modeling

#### Dimensional Modeling
- **Facts**: Measurable events (sales, clicks)
- **Dimensions**: Context (time, customer, product)
- **Star Schema**: Facts surrounded by dimensions
- **Slowly Changing Dimensions**: Track historical changes

#### Naming Conventions
```
Raw: raw__source__table
Staging: stg__source__table
Warehouse: dim/fct__entity__grain
```

### Pipeline Design
- **Idempotency**: Same input = same output
- **Atomicity**: All-or-nothing transactions
- **Fault tolerance**: Retry logic, dead letter queues
- **Backfilling**: Ability to reprocess historical data

### Cost Optimization
- **Partition pruning**: Query only relevant partitions
- **Clustering**: Co-locate related data
- **Materialized views**: Pre-compute common aggregations
- **Storage classes**: Move old data to cheaper storage

### Documentation
- **Data catalog**: Column descriptions, owners
- **Lineage**: Source to destination tracking
- **Runbooks**: Troubleshooting procedures

## ‚ö†Ô∏è Common Pitfalls

1. **No Data Quality Checks**: Trusting source data
2. **Over-Engineering**: Complex solutions for simple problems
3. **Ignoring Costs**: Surprise cloud bills
4. **Tight Coupling**: Pipelines that break easily
5. **No Documentation**: Tribal knowledge
6. **Manual Processes**: Not automating deployments
7. **Schema Drift**: Not handling source changes
8. **Ignoring Privacy**: GDPR, CCPA compliance
9. **Monolithic Pipelines**: Can't retry partial failures
10. **No Monitoring**: Finding out about issues from users

## üìä Data Architecture Patterns

### Lambda Architecture
```
Raw Data ‚Üí [Batch Layer] ‚Üí Batch Views
         ‚Üí [Speed Layer] ‚Üí Real-time Views
         ‚Üí [Serving Layer] ‚Üí Query
```

### Kappa Architecture
```
Raw Data ‚Üí Stream Processing ‚Üí Serving
         (Unified processing for both batch and streaming)
```

### medallion Architecture (Delta Lake)
```
Bronze (Raw) ‚Üí Silver (Cleaned) ‚Üí Gold (Curated)
```

## üîß Installation

### Universal
```
Read https://awesome-skills.dev/skills/data/data-engineer.md and apply
```

### OpenClaw
```bash
mkdir -p ~/.openclaw/skills/data-engineer
curl -o ~/.openclaw/skills/data-engineer/SKILL.md \
  https://awesome-skills.dev/skills/data/data-engineer.md
```

---

**Author**: Awesome Skills  
**Version**: 1.0.0  
**Updated**: 2026-02-16  
**Platforms**: Universal
